{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open('data/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            \n",
    "stopwords=[\"\", \"new\",\"non\",\"using\",\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\tourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n",
    "\n",
    "def pre_process_abstract(s):\n",
    "    s = re.split('\\W+', s)\n",
    "    s = [word.lower() for word in s if word.lower() not in stopwords]\n",
    "    return s\n",
    "\n",
    "def doc_list_to_sparse(words_list):\n",
    "    word_set = set([])\n",
    "    for words in words_list:\n",
    "        word_set.update(words)\n",
    "    word_dict = {}\n",
    "    for i, word in enumerate(word_set):\n",
    "        word_dict[word] = i\n",
    "        \n",
    "    indices = []\n",
    "    for i in tqdm(range(len(words_list))):\n",
    "        words = words_list[i]\n",
    "        for word in words:\n",
    "            indices.append([i, word_dict[word]])\n",
    "    \n",
    "    indices = torch.from_numpy(np.asarray(indices)).long()\n",
    "    values = torch.ones(len(indices))\n",
    "    size = torch.Size([len(words_list), len(word_set)])\n",
    "    print(indices.size())\n",
    "    print(values.size())\n",
    "    print(size)\n",
    "    return torch.sparse.FloatTensor(indices.t(), values, size), word_dict\n",
    "\n",
    "def doc_list_to_tf_idf(words_list, idf_words_idx):\n",
    "    idf_word_counter = Counter()\n",
    "    for words in [words_list[i] for i in idf_words_idx]:\n",
    "        idf_word_counter.update(list(set(words)))\n",
    "    \n",
    "    word_set = set(list(idf_word_counter.keys()))\n",
    "    \n",
    "    word_dict = {}\n",
    "    idf = {}\n",
    "    for i, word in enumerate(word_set):\n",
    "        word_dict[word] = i\n",
    "        idf[word] = np.log(np.asarray([float(len(idf_words_idx)) / idf_word_counter[word]]))\n",
    "        \n",
    "    indices = []\n",
    "    values = []\n",
    "    for i in tqdm(range(len(words_list))):\n",
    "        words = words_list[i]\n",
    "        num_word = 0\n",
    "        for word in words:\n",
    "            if word not in word_set:\n",
    "                continue\n",
    "            num_word += 1\n",
    "        for word in words:\n",
    "            if word not in word_set:\n",
    "                continue\n",
    "            indices.append([i, word_dict[word]])\n",
    "            values.append(1.0 / num_word * idf[word])\n",
    "    \n",
    "    indices = torch.from_numpy(np.asarray(indices)).long()\n",
    "    values = torch.from_numpy(np.asarray(values)).float().squeeze()\n",
    "    size = torch.Size([len(words_list), len(word_set)])\n",
    "    return torch.sparse.FloatTensor(indices.t(), values, size), word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796911\n"
     ]
    }
   ],
   "source": [
    "metadata = get_metadata()\n",
    "total_num = 0\n",
    "for paper in metadata:\n",
    "    total_num += 1\n",
    "print(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 74734/1796911 [00:25<09:38, 2978.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1420554a967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mabstracts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bfbaaacb110f>\u001b[0m in \u001b[0;36mpre_process_abstract\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_process_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\W+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bfbaaacb110f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_process_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\W+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load abstracts, categories, created_dates\n",
    "abstracts = []\n",
    "categories = []\n",
    "created_dates = []\n",
    "\n",
    "metadata = get_metadata()\n",
    "for paper in tqdm(metadata, total=total_num):\n",
    "    parsed = json.loads(paper)\n",
    "    abstract = pre_process_abstract(parsed[\"abstract\"])\n",
    "    abstracts.append(abstract)\n",
    "    categories.append(parsed[\"categories\"])\n",
    "    created_dates.append(parsed[\"versions\"][0]['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {}\n",
    "checkpoint[\"abstracts\"] = abstracts\n",
    "checkpoint[\"categories\"] = categories\n",
    "checkpoint[\"created_dates\"] = created_dates\n",
    "torch.save(checkpoint, \"data/arxiv_all.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"data/arxiv_all.pt\")\n",
    "abstracts = checkpoint[\"abstracts\"]\n",
    "categories = checkpoint[\"categories\"]\n",
    "created_dates = checkpoint[\"created_dates\"]\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1796911/1796911 [00:26<00:00, 68427.06it/s] \n"
     ]
    }
   ],
   "source": [
    "abstract_word_counter = Counter()\n",
    "for abstract in tqdm(abstracts, total=len(abstracts)):\n",
    "    abstract_word_counter.update(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1796911/1796911 [01:21<00:00, 21918.22it/s]\n"
     ]
    }
   ],
   "source": [
    "final_abstracts = []\n",
    "WordcountCutoff= 30\n",
    "abstract_word_counter_final = Counter()\n",
    "for abstract in tqdm(abstracts, total=len(abstracts)):\n",
    "    final_abstract = [word for word in abstract if abstract_word_counter[word] > WordcountCutoff]\n",
    "    final_abstracts.append(final_abstract)\n",
    "    abstract_word_counter_final.update(final_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1796911/1796911 [00:21<00:00, 82026.20it/s] \n"
     ]
    }
   ],
   "source": [
    "final_categories = []\n",
    "category_counter = Counter()\n",
    "for category in tqdm(categories):\n",
    "    category_counter.update(category.split(\" \"))\n",
    "    final_categories.append(category.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cs\n",
    "category_list = list(category_counter.keys())\n",
    "select_category_list = [category for category in category_list if (category.startswith(\"cs\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1796911it [00:01, 991221.33it/s] \n"
     ]
    }
   ],
   "source": [
    "filtered_abstracts = []\n",
    "filtered_categories = []\n",
    "filtered_full_categories = []\n",
    "filtered_date = []\n",
    "for (abstract, category, date) in tqdm(zip(final_abstracts, final_categories, created_dates)):\n",
    "    if len(abstract) <= 20:\n",
    "        continue\n",
    "    if category[0] not in select_category_list:\n",
    "        continue\n",
    "    filtered_abstracts.append(abstract)\n",
    "    filtered_categories.append(category[0])\n",
    "    filtered_full_categories.append(category)\n",
    "    filtered_date.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257062/257062 [00:00<00:00, 328612.55it/s]\n"
     ]
    }
   ],
   "source": [
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "months_to_val = {}\n",
    "for i, month in enumerate(months):\n",
    "    months_to_val[month] = i\n",
    "\n",
    "date_val = []\n",
    "for date in tqdm(filtered_date):\n",
    "    d_m_y_h_m_s = date.split(\" \")[1:4] + date.split(\" \")[4].split(\":\")\n",
    "    date_val.append(int(d_m_y_h_m_s[5]) + int(d_m_y_h_m_s[4]) * 60 + int(d_m_y_h_m_s[3]) * 60 * 60 + int(d_m_y_h_m_s[0]) * 30 * 60 * 60 + months_to_val[d_m_y_h_m_s[1]] * 40 * 30 * 60 * 60 + int(d_m_y_h_m_s[2]) * 20 * 40 * 30 * 60 * 60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = np.argsort(np.asarray(date_val))\n",
    "sorted_abstracts = [filtered_abstracts[i] for i in rank]\n",
    "sorted_categories = [filtered_categories[i] for i in rank]\n",
    "sorted_full_categories = [filtered_full_categories[i] for i in rank]\n",
    "sorted_date = [filtered_date[i] for i in rank]\n",
    "sorted_date_val = np.asarray(date_val)[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257062/257062 [01:29<00:00, 2876.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26064580, 2])\n",
      "torch.Size([26064580])\n",
      "torch.Size([257062, 44393])\n"
     ]
    }
   ],
   "source": [
    "abstracts_bow, word_dict = doc_list_to_sparse(sorted_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "categories_list = list(set(filtered_categories))\n",
    "categories_dict = {}\n",
    "for i, category in enumerate(categories_list):\n",
    "     categories_dict[category] = i\n",
    "categories_label = [categories_dict[category] for category in sorted_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remain_classes = []\n",
    "num_classes = 40\n",
    "for i in range(num_classes):\n",
    "    if (np.asarray(categories_label) == i).sum() < 3000:\n",
    "#         print(list(categories_dict.keys())[i])\n",
    "        continue\n",
    "    else:\n",
    "        remain_classes.append(i)\n",
    "remain_data_id = np.asarray([label in remain_classes for label in categories_label]).astype(np.bool)\n",
    "categories_label_np = np.asarray(categories_label)\n",
    "for i, class_id in enumerate(remain_classes):\n",
    "    categories_label_np[categories_label_np == class_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257062/257062 [02:14<00:00, 1909.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# alpha, beta = int(1 / 3 * len(categories_label_np[remain_data_id])), int(1.0 / 2 * len(categories_label_np[remain_data_id]))\n",
    "alpha, beta = int(1 / 2 * len(categories_label_np[remain_data_id])), int(3.0 / 4 * len(categories_label_np[remain_data_id]))\n",
    "np.random.seed(0)\n",
    "train_val_split = np.arange(len(categories_label_np)).astype(np.int)[remain_data_id][:beta].tolist()\n",
    "abstracts_tfidf, word_dict = doc_list_to_tf_idf(sorted_abstracts, train_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_checkpoint = {}\n",
    "raw_checkpoint[\"sorted_abstracts\"] = sorted_abstracts\n",
    "raw_checkpoint[\"sorted_categories\"] = sorted_categories\n",
    "raw_checkpoint[\"sorted_date\"] = sorted_date\n",
    "raw_checkpoint[\"sorted_date_val\"] = sorted_date_val\n",
    "raw_checkpoint[\"word_dict\"] = word_dict\n",
    "raw_checkpoint[\"categories_dict\"] = categories_dict\n",
    "raw_checkpoint[\"abstracts_bow\"] = abstracts_bow\n",
    "raw_checkpoint[\"categories_label\"] = categories_label\n",
    "raw_checkpoint[\"abstracts_tfidf\"] = abstracts_tfidf\n",
    "raw_checkpoint[\"sorted_full_categories\"] = sorted_full_categories\n",
    "torch.save(raw_checkpoint, \"data/arxiv_before_split.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_checkpoint = torch.load(\"data/arxiv_before_split.pt\")\n",
    "sorted_abstracts = raw_checkpoint[\"sorted_abstracts\"]\n",
    "abstracts_bow = raw_checkpoint[\"abstracts_bow\"]\n",
    "categories_label = np.asarray(raw_checkpoint[\"categories_label\"])\n",
    "categories_dict = raw_checkpoint[\"categories_dict\"]\n",
    "abstracts_tfidf = raw_checkpoint[\"abstracts_tfidf\"]\n",
    "del raw_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs.DB\n",
      "cs.AI\n",
      "cs.RO\n",
      "cs.DM\n",
      "cs.SI\n",
      "cs.DS\n",
      "cs.CR\n",
      "cs.HC\n",
      "cs.CL\n",
      "cs.LO\n",
      "cs.CC\n",
      "cs.GT\n",
      "cs.CV\n",
      "cs.PL\n",
      "cs.SY\n",
      "cs.DC\n",
      "cs.NI\n",
      "cs.IT\n",
      "cs.CY\n",
      "cs.LG\n",
      "cs.NE\n",
      "cs.IR\n",
      "cs.SE\n",
      "(257062,)\n"
     ]
    }
   ],
   "source": [
    "remain_classes = []\n",
    "num_classes = 40\n",
    "for i in range(num_classes):\n",
    "    if (np.asarray(categories_label) == i).sum() < 3000:\n",
    "        continue\n",
    "    else:\n",
    "        remain_classes.append(i)\n",
    "        print(list(categories_dict.keys())[i])\n",
    "remain_data_id = np.asarray([label in remain_classes for label in categories_label]).astype(np.bool)\n",
    "print(remain_data_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories_label_np = np.asarray(categories_label)\n",
    "for i, class_id in enumerate(remain_classes):\n",
    "    categories_label_np[categories_label_np == class_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_tfidf_dict = {\"indices\":abstracts_tfidf._indices().numpy(), \"values\": abstracts_tfidf._values().numpy(), \"size\": list(abstracts_tfidf.size())}\n",
    "values, indices, size = abstracts_tfidf_dict[\"values\"], abstracts_tfidf_dict[\"indices\"], abstracts_tfidf_dict[\"size\"]\n",
    "np_sparse_tfidf_dict = sparse.coo_matrix((values, (indices[0], indices[1])), size).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {}\n",
    "\n",
    "alpha, beta = int(1 / 2 * len(categories_label_np[remain_data_id])), int(3.0 / 4 * len(categories_label_np[remain_data_id]))\n",
    "\n",
    "np.random.seed(0)\n",
    "train_val_split = np.arange(len(categories_label_np)).astype(np.int)[remain_data_id][:beta]#[np.random.permutation(beta)]\n",
    "test_split = np.arange(len(categories_label_np)).astype(np.int)[remain_data_id][beta:]\n",
    "\n",
    "checkpoint[\"train_x\"] = np_sparse_tfidf_dict[train_val_split[:alpha]]\n",
    "checkpoint[\"train_y\"] = np.asarray(categories_label_np)[train_val_split[:alpha]]\n",
    "checkpoint[\"val_x\"] = np_sparse_tfidf_dict[train_val_split[alpha:]]\n",
    "checkpoint[\"val_y\"] = np.asarray(categories_label_np)[train_val_split[alpha:]]\n",
    "checkpoint[\"test_x\"] = np_sparse_tfidf_dict[test_split]\n",
    "checkpoint[\"test_y\"] = np.asarray(categories_label_np)[test_split]\n",
    "checkpoint[\"num_classes\"] = len(remain_classes)\n",
    "checkpoint[\"remain_classes\"] = remain_classes\n",
    "torch.save(checkpoint, \"data/arxiv.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116874, 41942)\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint[\"train_x\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233748\n"
     ]
    }
   ],
   "source": [
    "print(116874*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfiattack2",
   "language": "python",
   "name": "dfiattack2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
